{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    " \n",
    "from sklearn import metrics\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from lob_data_utils import lob, db_result, model, stocks_numbers\n",
    "from lob_data_utils.svm_calculation import lob_svm\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = 24000\n",
    "stocks = stocks_numbers.chosen_stocks\n",
    "should_save_fig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stocks = {}\n",
    "d_cv_stocks = {}\n",
    "d_test_stocks = {}\n",
    "for s in stocks:\n",
    "    d, d_test = lob.load_prepared_data(\n",
    "        s, data_dir='../data/prepared', length=data_length)\n",
    "    d.index = pd.to_datetime(d['Unnamed: 0'].values)\n",
    "    d_test.index = pd.to_datetime(d_test['Unnamed: 0'].values)\n",
    "    d_stocks[s] = d\n",
    "    d_test_stocks[s] = d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates of datasets\n",
      "9061\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "3459\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "4549\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:48:00\n",
      "testing 2013-10-30 12:49:00 - 2013-11-14 10:08:00\n",
      "9761\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "4851\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:52:00\n",
      "testing 2013-10-30 12:53:00 - 2013-11-14 10:12:00\n",
      "9062\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "11869\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:53:00\n",
      "testing 2013-10-30 12:54:00 - 2013-11-14 10:13:00\n",
      "12255\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:51:00\n",
      "testing 2013-10-30 12:52:00 - 2013-11-14 10:14:00\n",
      "2748\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:51:00\n",
      "testing 2013-10-30 12:52:00 - 2013-11-14 10:10:00\n",
      "4320\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:51:00\n",
      "testing 2013-10-30 12:52:00 - 2013-11-14 10:11:00\n",
      "11583\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "4799\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 13:02:00\n",
      "testing 2013-10-30 13:03:00 - 2013-11-14 10:27:00\n",
      "9268\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n",
      "10470\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:07:00\n",
      "9058\n",
      "training 2013-09-02 08:30:00 - 2013-10-30 12:47:00\n",
      "testing 2013-10-30 12:48:00 - 2013-11-14 10:06:00\n"
     ]
    }
   ],
   "source": [
    "print('Dates of datasets')\n",
    "for s in stocks:\n",
    "    print(s)\n",
    "    print('training', min(d_stocks[s].index), '-', max(d_stocks[s].index))\n",
    "    print('testing', min(d_test_stocks[s].index), '-', max(d_test_stocks[s].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on queue imbalance feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "def get_classes_weights(df):\n",
    "    y_train = df['mid_price_indicator'].values\n",
    "    classes = np.unique(y_train)\n",
    "    class_weight_list = utils.class_weight.compute_class_weight('balanced', classes, y_train)\n",
    "    class_weights = {classes[0]: class_weight_list[0], classes[1]: class_weight_list[1]}\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_dict_for_data(functions_to_run, dfs, clf, stock):\n",
    "    scores = {'stock': stock}\n",
    "    for func_name, func in functions_to_run.items():\n",
    "        for df_name, df in dfs.items():\n",
    "            pred = clf.predict(df['queue_imbalance'].values.reshape(-1, 1))\n",
    "            scores['{}_{}'.format(df_name, func_name)] = func(df['mid_price_indicator'], pred)\n",
    "    return scores\n",
    "            \n",
    "functions_to_run = {'precision': metrics.precision_score, 'roc_auc': metrics.roc_auc_score,\n",
    "                   'f1_score': metrics.f1_score, 'recall': metrics.recall_score,\n",
    "                   'matthews': metrics.matthews_corrcoef, 'kappa': metrics.cohen_kappa_score}\n",
    "scores = []\n",
    "for stock in stocks:\n",
    "    log_clf = LogisticRegression(random_state=0, class_weight=get_classes_weights(d_stocks[stock])) #21312)\n",
    "    res_train = model.validate_model(log_clf, d_stocks[stock][['queue_imbalance']], \n",
    "                         d_stocks[stock]['mid_price_indicator'])\n",
    "    dfs = {'test': d_test_stocks[stock]}\n",
    "    res = get_scores_dict_for_data(functions_to_run, dfs, log_clf, stock)\n",
    "    d_stocks[stock]['pred_log'] = log_clf.predict(d_stocks[stock][['queue_imbalance']])\n",
    "\n",
    "    res = {**res, **res_train}\n",
    "    scores.append(res)\n",
    "df_scores = pd.DataFrame(scores, index=stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>kappa</th>\n",
       "      <th>matthews</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>stock</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_kappa</th>\n",
       "      <th>test_matthews</th>\n",
       "      <th>...</th>\n",
       "      <th>train_matthews</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_val_f1</th>\n",
       "      <th>train_val_kappa</th>\n",
       "      <th>train_val_matthews</th>\n",
       "      <th>train_val_precision</th>\n",
       "      <th>train_val_recall</th>\n",
       "      <th>train_val_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>0.507121</td>\n",
       "      <td>0.094602</td>\n",
       "      <td>0.095738</td>\n",
       "      <td>0.485178</td>\n",
       "      <td>0.536079</td>\n",
       "      <td>0.547950</td>\n",
       "      <td>9061</td>\n",
       "      <td>0.511161</td>\n",
       "      <td>0.089713</td>\n",
       "      <td>0.090966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084134</td>\n",
       "      <td>0.474847</td>\n",
       "      <td>0.550240</td>\n",
       "      <td>0.542446</td>\n",
       "      <td>[0.5003092911047878, 0.48793634496919913, 0.49...</td>\n",
       "      <td>[0.07275227788965855, 0.0761604535558511, 0.08...</td>\n",
       "      <td>[0.0733284992740886, 0.07638647672456578, 0.08...</td>\n",
       "      <td>[0.46870653685674546, 0.46799606105366814, 0.4...</td>\n",
       "      <td>[0.5364818254178827, 0.5096514745308312, 0.516...</td>\n",
       "      <td>[0.5370060458808542, 0.5385094209490993, 0.540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>0.344387</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>0.120196</td>\n",
       "      <td>0.236054</td>\n",
       "      <td>0.641646</td>\n",
       "      <td>0.576229</td>\n",
       "      <td>3459</td>\n",
       "      <td>0.352005</td>\n",
       "      <td>0.112207</td>\n",
       "      <td>0.130774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129963</td>\n",
       "      <td>0.250888</td>\n",
       "      <td>0.600319</td>\n",
       "      <td>0.581691</td>\n",
       "      <td>[0.36654569449026725, 0.371033360455655, 0.361...</td>\n",
       "      <td>[0.11206933746230985, 0.11422034093170752, 0.1...</td>\n",
       "      <td>[0.1383540326726628, 0.14171977601535152, 0.14...</td>\n",
       "      <td>[0.2584321935333799, 0.2612282309807516, 0.257...</td>\n",
       "      <td>[0.6301758366420873, 0.6400898371701291, 0.605...</td>\n",
       "      <td>[0.5861307221130574, 0.5879020614422075, 0.588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>0.435926</td>\n",
       "      <td>0.112035</td>\n",
       "      <td>0.122816</td>\n",
       "      <td>0.348114</td>\n",
       "      <td>0.586051</td>\n",
       "      <td>0.567216</td>\n",
       "      <td>4549</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>0.123603</td>\n",
       "      <td>0.133548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119528</td>\n",
       "      <td>0.348725</td>\n",
       "      <td>0.569006</td>\n",
       "      <td>0.565663</td>\n",
       "      <td>[0.4273014916377882, 0.4255255255255255, 0.427...</td>\n",
       "      <td>[0.10553134626982785, 0.10182457803431133, 0.1...</td>\n",
       "      <td>[0.11328400925629142, 0.10955769730021216, 0.1...</td>\n",
       "      <td>[0.3466145196773405, 0.34426627793974734, 0.34...</td>\n",
       "      <td>[0.5569520816967792, 0.5569968553459119, 0.549...</td>\n",
       "      <td>[0.5621789053895775, 0.5601680604184665, 0.571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>0.428456</td>\n",
       "      <td>0.140536</td>\n",
       "      <td>0.146427</td>\n",
       "      <td>0.363742</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.579966</td>\n",
       "      <td>9761</td>\n",
       "      <td>0.436612</td>\n",
       "      <td>0.099671</td>\n",
       "      <td>0.107514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.355845</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.568539</td>\n",
       "      <td>[0.4509911894273127, 0.4249655805415329, 0.408...</td>\n",
       "      <td>[0.10526650074146915, 0.10701413455414643, 0.1...</td>\n",
       "      <td>[0.11700738710866163, 0.11404332075676671, 0.1...</td>\n",
       "      <td>[0.355700325732899, 0.3475975975975976, 0.3449...</td>\n",
       "      <td>[0.6160210605490786, 0.5466351829988194, 0.501...</td>\n",
       "      <td>[0.5634506885336886, 0.5625316924049725, 0.563...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>0.290029</td>\n",
       "      <td>0.103769</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.189234</td>\n",
       "      <td>0.631029</td>\n",
       "      <td>0.602119</td>\n",
       "      <td>4851</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>0.103878</td>\n",
       "      <td>0.138680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146122</td>\n",
       "      <td>0.208106</td>\n",
       "      <td>0.611383</td>\n",
       "      <td>0.601755</td>\n",
       "      <td>[0.3206958805186279, 0.32526881720430106, 0.32...</td>\n",
       "      <td>[0.09425063903595143, 0.10250756204922207, 0.1...</td>\n",
       "      <td>[0.133946046494205, 0.14240065482781297, 0.159...</td>\n",
       "      <td>[0.20920770877944325, 0.21373371605210864, 0.2...</td>\n",
       "      <td>[0.6865776528460998, 0.6802529866479269, 0.647...</td>\n",
       "      <td>[0.5904129020187772, 0.5962891884759766, 0.610...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>0.493426</td>\n",
       "      <td>0.057121</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.481162</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.528676</td>\n",
       "      <td>9062</td>\n",
       "      <td>0.499015</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.480502</td>\n",
       "      <td>0.532499</td>\n",
       "      <td>0.531175</td>\n",
       "      <td>[0.5064283090486102, 0.5042181195745201, 0.492...</td>\n",
       "      <td>[0.07369896033659906, 0.06846937524326724, 0.0...</td>\n",
       "      <td>[0.07393986748500468, 0.06871134557089326, 0.0...</td>\n",
       "      <td>[0.485559990608124, 0.48267790262172283, 0.477...</td>\n",
       "      <td>[0.5291709314227226, 0.5277706680317379, 0.507...</td>\n",
       "      <td>[0.5371617580094722, 0.5345370567223862, 0.530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11869</th>\n",
       "      <td>0.422944</td>\n",
       "      <td>0.107128</td>\n",
       "      <td>0.120455</td>\n",
       "      <td>0.332089</td>\n",
       "      <td>0.601182</td>\n",
       "      <td>0.566097</td>\n",
       "      <td>11869</td>\n",
       "      <td>0.411849</td>\n",
       "      <td>0.099842</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119060</td>\n",
       "      <td>0.324760</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>0.567023</td>\n",
       "      <td>[0.39396536944968286, 0.39024390243902446, 0.4...</td>\n",
       "      <td>[0.11265978079273353, 0.11021337694023703, 0.1...</td>\n",
       "      <td>[0.11946073912096442, 0.11659852263493942, 0.1...</td>\n",
       "      <td>[0.32103939647946356, 0.3189143341815098, 0.32...</td>\n",
       "      <td>[0.5097604259094942, 0.5026737967914439, 0.543...</td>\n",
       "      <td>[0.56711943409522, 0.5654859494680873, 0.56487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>0.422225</td>\n",
       "      <td>0.129384</td>\n",
       "      <td>0.140048</td>\n",
       "      <td>0.337824</td>\n",
       "      <td>0.566759</td>\n",
       "      <td>0.578323</td>\n",
       "      <td>12255</td>\n",
       "      <td>0.412012</td>\n",
       "      <td>0.102106</td>\n",
       "      <td>0.111629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.336310</td>\n",
       "      <td>0.572098</td>\n",
       "      <td>0.568951</td>\n",
       "      <td>[0.4339131625804684, 0.42269253380364485, 0.41...</td>\n",
       "      <td>[0.08918625509854561, 0.09873889504512312, 0.1...</td>\n",
       "      <td>[0.1027751349527756, 0.10974032194937747, 0.12...</td>\n",
       "      <td>[0.33151946421096695, 0.33034688720422695, 0.3...</td>\n",
       "      <td>[0.6278240190249703, 0.5866993064055488, 0.552...</td>\n",
       "      <td>[0.5564144280871385, 0.5610417156828509, 0.568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.275834</td>\n",
       "      <td>0.115930</td>\n",
       "      <td>0.143627</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.496546</td>\n",
       "      <td>0.592444</td>\n",
       "      <td>2748</td>\n",
       "      <td>0.291378</td>\n",
       "      <td>0.090305</td>\n",
       "      <td>0.134270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105855</td>\n",
       "      <td>0.184566</td>\n",
       "      <td>0.580131</td>\n",
       "      <td>0.575097</td>\n",
       "      <td>[0.26843878697117185, 0.2090909090909091, 0.22...</td>\n",
       "      <td>[0.06169525741580861, 0.09248273018217901, 0.0...</td>\n",
       "      <td>[0.08403748562839436, 0.0973096088719295, 0.10...</td>\n",
       "      <td>[0.17552019583843328, 0.1644993498049415, 0.17...</td>\n",
       "      <td>[0.5704057279236276, 0.2868480725623583, 0.352...</td>\n",
       "      <td>[0.5597074167202498, 0.5615038526779661, 0.571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>0.454826</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.135224</td>\n",
       "      <td>0.367295</td>\n",
       "      <td>0.598012</td>\n",
       "      <td>0.573640</td>\n",
       "      <td>4320</td>\n",
       "      <td>0.449637</td>\n",
       "      <td>0.125494</td>\n",
       "      <td>0.133149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>0.378807</td>\n",
       "      <td>0.591625</td>\n",
       "      <td>0.576916</td>\n",
       "      <td>[0.48031389527804086, 0.47440772155601046, 0.4...</td>\n",
       "      <td>[0.13480223757937393, 0.1594867445736462, 0.16...</td>\n",
       "      <td>[0.1469433350118735, 0.16926066429988681, 0.16...</td>\n",
       "      <td>[0.38763922253767197, 0.39226118500604595, 0.3...</td>\n",
       "      <td>[0.6312233285917497, 0.6000739918608953, 0.537...</td>\n",
       "      <td>[0.5785069797026058, 0.591385385169903, 0.5903...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11583</th>\n",
       "      <td>0.474162</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.103811</td>\n",
       "      <td>0.414241</td>\n",
       "      <td>0.555686</td>\n",
       "      <td>0.553847</td>\n",
       "      <td>11583</td>\n",
       "      <td>0.463001</td>\n",
       "      <td>0.106195</td>\n",
       "      <td>0.108526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100042</td>\n",
       "      <td>0.409878</td>\n",
       "      <td>0.560058</td>\n",
       "      <td>0.552063</td>\n",
       "      <td>[0.4715358361774744, 0.47196697296577445, 0.47...</td>\n",
       "      <td>[0.10313752199803505, 0.0906844149189302, 0.08...</td>\n",
       "      <td>[0.10648036139986854, 0.09433317033940794, 0.0...</td>\n",
       "      <td>[0.4112884020004763, 0.4068886337543054, 0.407...</td>\n",
       "      <td>[0.5524632117722329, 0.5618262523779328, 0.572...</td>\n",
       "      <td>[0.5554778998671845, 0.5490876011485787, 0.547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.416204</td>\n",
       "      <td>0.107205</td>\n",
       "      <td>0.123985</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.619860</td>\n",
       "      <td>0.569671</td>\n",
       "      <td>4799</td>\n",
       "      <td>0.410569</td>\n",
       "      <td>0.114490</td>\n",
       "      <td>0.124435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.324848</td>\n",
       "      <td>0.571783</td>\n",
       "      <td>0.562566</td>\n",
       "      <td>[0.4337317397078353, 0.4235261010271563, 0.420...</td>\n",
       "      <td>[0.08171333078342846, 0.087649058695209, 0.104...</td>\n",
       "      <td>[0.09722208881579789, 0.10058467380751693, 0.1...</td>\n",
       "      <td>[0.32510451921162653, 0.3233082706766917, 0.32...</td>\n",
       "      <td>[0.6513761467889908, 0.613784665579119, 0.5904...</td>\n",
       "      <td>[0.5530923834620411, 0.5558162926173931, 0.565...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.429318</td>\n",
       "      <td>0.109560</td>\n",
       "      <td>0.116182</td>\n",
       "      <td>0.354562</td>\n",
       "      <td>0.544722</td>\n",
       "      <td>0.563328</td>\n",
       "      <td>9268</td>\n",
       "      <td>0.455886</td>\n",
       "      <td>0.122201</td>\n",
       "      <td>0.132175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122804</td>\n",
       "      <td>0.366101</td>\n",
       "      <td>0.564870</td>\n",
       "      <td>0.566503</td>\n",
       "      <td>[0.46559538524927896, 0.44929889298892983, 0.4...</td>\n",
       "      <td>[0.1203107617434479, 0.12525541618276848, 0.12...</td>\n",
       "      <td>[0.13132272935561157, 0.13319997013049167, 0.1...</td>\n",
       "      <td>[0.3744201457919152, 0.3700461949914904, 0.362...</td>\n",
       "      <td>[0.6154684095860566, 0.5717505634861006, 0.531...</td>\n",
       "      <td>[0.5705879981274781, 0.572202182617205, 0.5686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10470</th>\n",
       "      <td>0.401060</td>\n",
       "      <td>0.124735</td>\n",
       "      <td>0.136879</td>\n",
       "      <td>0.316368</td>\n",
       "      <td>0.556429</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>10470</td>\n",
       "      <td>0.412347</td>\n",
       "      <td>0.117208</td>\n",
       "      <td>0.126946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129243</td>\n",
       "      <td>0.310112</td>\n",
       "      <td>0.596408</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>[0.4125863843552419, 0.39312430367658757, 0.38...</td>\n",
       "      <td>[0.10247870128602743, 0.10075749968476366, 0.1...</td>\n",
       "      <td>[0.11952665297664239, 0.11357768295238592, 0.1...</td>\n",
       "      <td>[0.3086229652441707, 0.29975728155339804, 0.29...</td>\n",
       "      <td>[0.6221729490022173, 0.5709662505779011, 0.527...</td>\n",
       "      <td>[0.5681962117499494, 0.5656568528110474, 0.573...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>0.483996</td>\n",
       "      <td>0.073309</td>\n",
       "      <td>0.074776</td>\n",
       "      <td>0.436993</td>\n",
       "      <td>0.542896</td>\n",
       "      <td>0.538148</td>\n",
       "      <td>9058</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.089998</td>\n",
       "      <td>0.091455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078873</td>\n",
       "      <td>0.439103</td>\n",
       "      <td>0.547737</td>\n",
       "      <td>0.540243</td>\n",
       "      <td>[0.4885885502996303, 0.4804804804804805, 0.488...</td>\n",
       "      <td>[0.08000628191381653, 0.08056707373803451, 0.0...</td>\n",
       "      <td>[0.08159060573467303, 0.08172852674250584, 0.0...</td>\n",
       "      <td>[0.4405610485169004, 0.43882661578821847, 0.44...</td>\n",
       "      <td>[0.5483686319404694, 0.5308713214079631, 0.545...</td>\n",
       "      <td>[0.5416283993194986, 0.5417239284354894, 0.543...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1     kappa  matthews  precision    recall   roc_auc  stock  \\\n",
       "9061   0.507121  0.094602  0.095738   0.485178  0.536079  0.547950   9061   \n",
       "3459   0.344387  0.092428  0.120196   0.236054  0.641646  0.576229   3459   \n",
       "4549   0.435926  0.112035  0.122816   0.348114  0.586051  0.567216   4549   \n",
       "9761   0.428456  0.140536  0.146427   0.363742  0.523900  0.579966   9761   \n",
       "4851   0.290029  0.103769  0.141509   0.189234  0.631029  0.602119   4851   \n",
       "9062   0.493426  0.057121  0.057241   0.481162  0.507614  0.528676   9062   \n",
       "11869  0.422944  0.107128  0.120455   0.332089  0.601182  0.566097  11869   \n",
       "12255  0.422225  0.129384  0.140048   0.337824  0.566759  0.578323  12255   \n",
       "2748   0.275834  0.115930  0.143627   0.234475  0.496546  0.592444   2748   \n",
       "4320   0.454826  0.124671  0.135224   0.367295  0.598012  0.573640   4320   \n",
       "11583  0.474162  0.100242  0.103811   0.414241  0.555686  0.553847  11583   \n",
       "4799   0.416204  0.107205  0.123985   0.315550  0.619860  0.569671   4799   \n",
       "9268   0.429318  0.109560  0.116182   0.354562  0.544722  0.563328   9268   \n",
       "10470  0.401060  0.124735  0.136879   0.316368  0.556429  0.578000  10470   \n",
       "9058   0.483996  0.073309  0.074776   0.436993  0.542896  0.538148   9058   \n",
       "\n",
       "       test_f1_score  test_kappa  test_matthews  \\\n",
       "9061        0.511161    0.089713       0.090966   \n",
       "3459        0.352005    0.112207       0.130774   \n",
       "4549        0.430924    0.123603       0.133548   \n",
       "9761        0.436612    0.099671       0.107514   \n",
       "4851        0.287706    0.103878       0.138680   \n",
       "9062        0.499015    0.047593       0.047951   \n",
       "11869       0.411849    0.099842       0.105285   \n",
       "12255       0.412012    0.102106       0.111629   \n",
       "2748        0.291378    0.090305       0.134270   \n",
       "4320        0.449637    0.125494       0.133149   \n",
       "11583       0.463001    0.106195       0.108526   \n",
       "4799        0.410569    0.114490       0.124435   \n",
       "9268        0.455886    0.122201       0.132175   \n",
       "10470       0.412347    0.117208       0.126946   \n",
       "9058        0.491228    0.089998       0.091455   \n",
       "\n",
       "                             ...                          train_matthews  \\\n",
       "9061                         ...                                0.084134   \n",
       "3459                         ...                                0.129963   \n",
       "4549                         ...                                0.119528   \n",
       "9761                         ...                                0.125276   \n",
       "4851                         ...                                0.146122   \n",
       "9062                         ...                                0.062033   \n",
       "11869                        ...                                0.119060   \n",
       "12255                        ...                                0.123718   \n",
       "2748                         ...                                0.105855   \n",
       "4320                         ...                                0.142469   \n",
       "11583                        ...                                0.100042   \n",
       "4799                         ...                                0.111598   \n",
       "9268                         ...                                0.122804   \n",
       "10470                        ...                                0.129243   \n",
       "9058                         ...                                0.078873   \n",
       "\n",
       "       train_precision  train_recall  train_roc_auc  \\\n",
       "9061          0.474847      0.550240       0.542446   \n",
       "3459          0.250888      0.600319       0.581691   \n",
       "4549          0.348725      0.569006       0.565663   \n",
       "9761          0.355845      0.572359       0.568539   \n",
       "4851          0.208106      0.611383       0.601755   \n",
       "9062          0.480502      0.532499       0.531175   \n",
       "11869         0.324760      0.569824       0.567023   \n",
       "12255         0.336310      0.572098       0.568951   \n",
       "2748          0.184566      0.580131       0.575097   \n",
       "4320          0.378807      0.591625       0.576916   \n",
       "11583         0.409878      0.560058       0.552063   \n",
       "4799          0.324848      0.571783       0.562566   \n",
       "9268          0.366101      0.564870       0.566503   \n",
       "10470         0.310112      0.596408       0.574360   \n",
       "9058          0.439103      0.547737       0.540243   \n",
       "\n",
       "                                            train_val_f1  \\\n",
       "9061   [0.5003092911047878, 0.48793634496919913, 0.49...   \n",
       "3459   [0.36654569449026725, 0.371033360455655, 0.361...   \n",
       "4549   [0.4273014916377882, 0.4255255255255255, 0.427...   \n",
       "9761   [0.4509911894273127, 0.4249655805415329, 0.408...   \n",
       "4851   [0.3206958805186279, 0.32526881720430106, 0.32...   \n",
       "9062   [0.5064283090486102, 0.5042181195745201, 0.492...   \n",
       "11869  [0.39396536944968286, 0.39024390243902446, 0.4...   \n",
       "12255  [0.4339131625804684, 0.42269253380364485, 0.41...   \n",
       "2748   [0.26843878697117185, 0.2090909090909091, 0.22...   \n",
       "4320   [0.48031389527804086, 0.47440772155601046, 0.4...   \n",
       "11583  [0.4715358361774744, 0.47196697296577445, 0.47...   \n",
       "4799   [0.4337317397078353, 0.4235261010271563, 0.420...   \n",
       "9268   [0.46559538524927896, 0.44929889298892983, 0.4...   \n",
       "10470  [0.4125863843552419, 0.39312430367658757, 0.38...   \n",
       "9058   [0.4885885502996303, 0.4804804804804805, 0.488...   \n",
       "\n",
       "                                         train_val_kappa  \\\n",
       "9061   [0.07275227788965855, 0.0761604535558511, 0.08...   \n",
       "3459   [0.11206933746230985, 0.11422034093170752, 0.1...   \n",
       "4549   [0.10553134626982785, 0.10182457803431133, 0.1...   \n",
       "9761   [0.10526650074146915, 0.10701413455414643, 0.1...   \n",
       "4851   [0.09425063903595143, 0.10250756204922207, 0.1...   \n",
       "9062   [0.07369896033659906, 0.06846937524326724, 0.0...   \n",
       "11869  [0.11265978079273353, 0.11021337694023703, 0.1...   \n",
       "12255  [0.08918625509854561, 0.09873889504512312, 0.1...   \n",
       "2748   [0.06169525741580861, 0.09248273018217901, 0.0...   \n",
       "4320   [0.13480223757937393, 0.1594867445736462, 0.16...   \n",
       "11583  [0.10313752199803505, 0.0906844149189302, 0.08...   \n",
       "4799   [0.08171333078342846, 0.087649058695209, 0.104...   \n",
       "9268   [0.1203107617434479, 0.12525541618276848, 0.12...   \n",
       "10470  [0.10247870128602743, 0.10075749968476366, 0.1...   \n",
       "9058   [0.08000628191381653, 0.08056707373803451, 0.0...   \n",
       "\n",
       "                                      train_val_matthews  \\\n",
       "9061   [0.0733284992740886, 0.07638647672456578, 0.08...   \n",
       "3459   [0.1383540326726628, 0.14171977601535152, 0.14...   \n",
       "4549   [0.11328400925629142, 0.10955769730021216, 0.1...   \n",
       "9761   [0.11700738710866163, 0.11404332075676671, 0.1...   \n",
       "4851   [0.133946046494205, 0.14240065482781297, 0.159...   \n",
       "9062   [0.07393986748500468, 0.06871134557089326, 0.0...   \n",
       "11869  [0.11946073912096442, 0.11659852263493942, 0.1...   \n",
       "12255  [0.1027751349527756, 0.10974032194937747, 0.12...   \n",
       "2748   [0.08403748562839436, 0.0973096088719295, 0.10...   \n",
       "4320   [0.1469433350118735, 0.16926066429988681, 0.16...   \n",
       "11583  [0.10648036139986854, 0.09433317033940794, 0.0...   \n",
       "4799   [0.09722208881579789, 0.10058467380751693, 0.1...   \n",
       "9268   [0.13132272935561157, 0.13319997013049167, 0.1...   \n",
       "10470  [0.11952665297664239, 0.11357768295238592, 0.1...   \n",
       "9058   [0.08159060573467303, 0.08172852674250584, 0.0...   \n",
       "\n",
       "                                     train_val_precision  \\\n",
       "9061   [0.46870653685674546, 0.46799606105366814, 0.4...   \n",
       "3459   [0.2584321935333799, 0.2612282309807516, 0.257...   \n",
       "4549   [0.3466145196773405, 0.34426627793974734, 0.34...   \n",
       "9761   [0.355700325732899, 0.3475975975975976, 0.3449...   \n",
       "4851   [0.20920770877944325, 0.21373371605210864, 0.2...   \n",
       "9062   [0.485559990608124, 0.48267790262172283, 0.477...   \n",
       "11869  [0.32103939647946356, 0.3189143341815098, 0.32...   \n",
       "12255  [0.33151946421096695, 0.33034688720422695, 0.3...   \n",
       "2748   [0.17552019583843328, 0.1644993498049415, 0.17...   \n",
       "4320   [0.38763922253767197, 0.39226118500604595, 0.3...   \n",
       "11583  [0.4112884020004763, 0.4068886337543054, 0.407...   \n",
       "4799   [0.32510451921162653, 0.3233082706766917, 0.32...   \n",
       "9268   [0.3744201457919152, 0.3700461949914904, 0.362...   \n",
       "10470  [0.3086229652441707, 0.29975728155339804, 0.29...   \n",
       "9058   [0.4405610485169004, 0.43882661578821847, 0.44...   \n",
       "\n",
       "                                        train_val_recall  \\\n",
       "9061   [0.5364818254178827, 0.5096514745308312, 0.516...   \n",
       "3459   [0.6301758366420873, 0.6400898371701291, 0.605...   \n",
       "4549   [0.5569520816967792, 0.5569968553459119, 0.549...   \n",
       "9761   [0.6160210605490786, 0.5466351829988194, 0.501...   \n",
       "4851   [0.6865776528460998, 0.6802529866479269, 0.647...   \n",
       "9062   [0.5291709314227226, 0.5277706680317379, 0.507...   \n",
       "11869  [0.5097604259094942, 0.5026737967914439, 0.543...   \n",
       "12255  [0.6278240190249703, 0.5866993064055488, 0.552...   \n",
       "2748   [0.5704057279236276, 0.2868480725623583, 0.352...   \n",
       "4320   [0.6312233285917497, 0.6000739918608953, 0.537...   \n",
       "11583  [0.5524632117722329, 0.5618262523779328, 0.572...   \n",
       "4799   [0.6513761467889908, 0.613784665579119, 0.5904...   \n",
       "9268   [0.6154684095860566, 0.5717505634861006, 0.531...   \n",
       "10470  [0.6221729490022173, 0.5709662505779011, 0.527...   \n",
       "9058   [0.5483686319404694, 0.5308713214079631, 0.545...   \n",
       "\n",
       "                                       train_val_roc_auc  \n",
       "9061   [0.5370060458808542, 0.5385094209490993, 0.540...  \n",
       "3459   [0.5861307221130574, 0.5879020614422075, 0.588...  \n",
       "4549   [0.5621789053895775, 0.5601680604184665, 0.571...  \n",
       "9761   [0.5634506885336886, 0.5625316924049725, 0.563...  \n",
       "4851   [0.5904129020187772, 0.5962891884759766, 0.610...  \n",
       "9062   [0.5371617580094722, 0.5345370567223862, 0.530...  \n",
       "11869  [0.56711943409522, 0.5654859494680873, 0.56487...  \n",
       "12255  [0.5564144280871385, 0.5610417156828509, 0.568...  \n",
       "2748   [0.5597074167202498, 0.5615038526779661, 0.571...  \n",
       "4320   [0.5785069797026058, 0.591385385169903, 0.5903...  \n",
       "11583  [0.5554778998671845, 0.5490876011485787, 0.547...  \n",
       "4799   [0.5530923834620411, 0.5558162926173931, 0.565...  \n",
       "9268   [0.5705879981274781, 0.572202182617205, 0.5686...  \n",
       "10470  [0.5681962117499494, 0.5656568528110474, 0.573...  \n",
       "9058   [0.5416283993194986, 0.5417239284354894, 0.543...  \n",
       "\n",
       "[15 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_scores(df, column):\n",
    "    scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        scores.append(np.mean(row[column]))\n",
    "    return scores\n",
    "scores_columns = ['f1', 'kappa', 'matthews', 'precision', 'recall', 'roc_auc', 'train_f1', 'train_kappa',\n",
    "       'train_matthews', 'train_precision', 'train_recall', 'train_roc_auc']\n",
    "\n",
    "for col in scores_columns:\n",
    "    df_scores[col] = convert_scores(df_scores, col)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pivot</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>0.023023</td>\n",
       "      <td>9061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>0.096860</td>\n",
       "      <td>3459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>0.032053</td>\n",
       "      <td>4549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>0.043341</td>\n",
       "      <td>9761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>0.089948</td>\n",
       "      <td>4851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>0.012105</td>\n",
       "      <td>9062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11869</th>\n",
       "      <td>0.037273</td>\n",
       "      <td>11869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>0.046640</td>\n",
       "      <td>12255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.038263</td>\n",
       "      <td>2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>0.053965</td>\n",
       "      <td>4320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11583</th>\n",
       "      <td>0.046703</td>\n",
       "      <td>11583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.049480</td>\n",
       "      <td>4799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.018577</td>\n",
       "      <td>9268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10470</th>\n",
       "      <td>0.085225</td>\n",
       "      <td>10470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>0.031014</td>\n",
       "      <td>9058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pivot  stock\n",
       "9061   0.023023   9061\n",
       "3459   0.096860   3459\n",
       "4549   0.032053   4549\n",
       "9761   0.043341   9761\n",
       "4851   0.089948   4851\n",
       "9062   0.012105   9062\n",
       "11869  0.037273  11869\n",
       "12255  0.046640  12255\n",
       "2748   0.038263   2748\n",
       "4320   0.053965   4320\n",
       "11583  0.046703  11583\n",
       "4799   0.049480   4799\n",
       "9268   0.018577   9268\n",
       "10470  0.085225  10470\n",
       "9058   0.031014   9058"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivots = []\n",
    "print('Pivot values')\n",
    "for i in df_scores.index:\n",
    "    stock = i\n",
    "    df = d_stocks[stock]\n",
    "    \n",
    "    pivot = np.mean([np.min(df[df['pred_log'] == 1]['queue_imbalance']), \n",
    "                    np.max(df[df['pred_log'] == 0]['queue_imbalance'])])\n",
    "    pivots.append(pivot)\n",
    "df_scores['pivot'] = pivots\n",
    "df_scores[['pivot', 'stock']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[['pivot', 'stock']].to_csv('que_log_pivot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, d in d_stocks.items():\n",
    "    above_pivot = []\n",
    "    pivot = df_scores[df_scores['stock'] == k]['pivot'].values[0]\n",
    "    for i, row in d.iterrows():\n",
    "        if row['queue_imbalance'] < pivot:\n",
    "            above_pivot.append(0)\n",
    "        else:\n",
    "            above_pivot.append(1)\n",
    "    d['above_pivot'] = above_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Positive and ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 3, sharey=True, sharex=True, figsize=(16, 4))\n",
    "i = 0\n",
    "\n",
    "for k, d in d_stocks.items():\n",
    "    pivot = df_scores[df_scores['stock'] == k]['pivot'].values[0]\n",
    "    df = d[d['queue_imbalance'] < pivot]\n",
    "    confusion_matrix1 = metrics.confusion_matrix(df['mid_price_indicator'], df['pred_log'])\n",
    "    ax[i].bar(height = confusion_matrix1.ravel(), x=['Negatives below pivot','Positives below pivot',\n",
    "                                                     'Negatives above pivot','Postives above pivot'])\n",
    "    df = d[d['queue_imbalance'] > pivot]\n",
    "    confusion_matrix2 = metrics.confusion_matrix(df['mid_price_indicator'], df['pred_log'])\n",
    "    ax[i].bar(height = confusion_matrix2.ravel(), x=['Negatives below pivot','Positives below pivot',\n",
    "                                                     'Negatives above pivot','Postives above pivot'])\n",
    "    i += 1\n",
    "plt.title('Positive and ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
